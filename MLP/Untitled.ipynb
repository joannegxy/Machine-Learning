{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b78e2c-68af-4ac1-aed1-ea0ed613ab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  0.8242474685799568 Valid Loss:  0.3361524888809691\n",
      "Epoch: 2 Training Loss:  0.31872991020692154 Valid Loss:  0.27585283143723266\n",
      "Epoch: 3 Training Loss:  0.26101052642185635 Valid Loss:  0.23362707394234677\n",
      "Epoch: 4 Training Loss:  0.2164407585688094 Valid Loss:  0.1906954306871333\n",
      "Epoch: 5 Training Loss:  0.17989287660159964 Valid Loss:  0.16836124848812184\n",
      "Epoch: 6 Training Loss:  0.15362315466429324 Valid Loss:  0.15042071789503098\n",
      "Epoch: 7 Training Loss:  0.13465994009946256 Valid Loss:  0.1370177834909013\n",
      "Epoch: 8 Training Loss:  0.11785116726334424 Valid Loss:  0.13118550514287136\n",
      "Epoch: 9 Training Loss:  0.1050559034808836 Valid Loss:  0.12004840445328266\n",
      "Epoch: 10 Training Loss:  0.09389746902470893 Valid Loss:  0.1152433730820392\n",
      "Actual: tensor([2, 3, 8, 0, 1, 7, 5, 6, 3, 9, 9, 0, 1, 6, 3, 5, 0, 9, 9, 2])\n",
      "Predicted: [2 3 8 8 1 7 5 6 3 9 9 0 1 6 3 5 0 9 9 7]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "_tasks = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "    \n",
    "mnist = MNIST(\"data\", download=True, train=True, transform=_tasks)\n",
    "\n",
    "split = int(0.8 * len(mnist))\n",
    "index_list = list(range(len(mnist)))\n",
    "train_idx, valid_idx = index_list[:split], index_list[split:]\n",
    "## create sampler objects using SubsetRandomSampler\n",
    "tr_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(valid_idx)\n",
    "## create iterator objects for train and valid datasets\n",
    "trainloader = DataLoader(mnist, batch_size=256, sampler=tr_sampler)\n",
    "validloader = DataLoader(mnist, batch_size=256, sampler=val_sampler)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "model = Model()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay= 1e-6, momentum = 0.9, nesterov = True)\n",
    "\n",
    "for epoch in range(1, 11): \n",
    "    train_loss, valid_loss = [], []\n",
    "    ## training part \n",
    "    model.train()\n",
    "    for data, target in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        ## 1. forward propagation\n",
    "        output = model(data)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = loss_function(output, target)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    ## evaluation part \n",
    "    model.eval()\n",
    "    for data, target in validloader:\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        valid_loss.append(loss.item())\n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))\n",
    "\n",
    "## dataloader for validation dataset \n",
    "dataiter = iter(validloader)\n",
    "data, labels = next(dataiter)\n",
    "output = model(data)\n",
    "_, preds_tensor = torch.max(output, 1)\n",
    "preds = np.squeeze(preds_tensor.numpy())\n",
    "print (\"Actual:\", labels[:20])\n",
    "print (\"Predicted:\", preds[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a20781-e815-46b9-bf6e-637a0be884ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
